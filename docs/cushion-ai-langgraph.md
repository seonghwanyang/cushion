# Cushion AI/LangGraph í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ ê°€ì´ë“œ

## ğŸ¤– ê°œìš”

ì´ ê°€ì´ë“œëŠ” LangGraphë¥¼ ì‚¬ìš©í•œ AI ì›Œí¬í”Œë¡œìš°ë¥¼ Docker/Kubernetes í™˜ê²½ì—ì„œ íš¨ìœ¨ì ìœ¼ë¡œ ìš´ì˜í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.

## ğŸ— AI ì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜

### 1. AI ì„œë¹„ìŠ¤ ë¶„ë¦¬ ì„¤ê³„

```typescript
// âŒ Bad: ëª¨ë†€ë¦¬ì‹ AI ì²˜ë¦¬
class DiaryService {
  async analyzeDiary(content: string) {
    // API ì„œë²„ì—ì„œ ì§ì ‘ AI ì²˜ë¦¬ (ëŠë¦¼, ë¹„ìš© ë†’ìŒ)
    const result = await openai.chat.completions.create({...});
    return result;
  }
}

// âœ… Good: AI ì›Œì»¤ ë¶„ë¦¬
// 1. API ì„œë²„ - ìš”ì²­ë§Œ ë°›ìŒ
class DiaryController {
  async analyzeDiary(req: Request, res: Response) {
    // íì— ì‘ì—… ì¶”ê°€
    const job = await aiQueue.add('analyze-diary', {
      diaryId: req.params.id,
      content: req.body.content,
      userId: req.user.id
    });
    
    // ì¦‰ì‹œ ì‘ë‹µ
    res.json({ 
      jobId: job.id, 
      status: 'processing',
      estimatedTime: 30 
    });
  }
}

// 2. AI ì›Œì»¤ - ë³„ë„ í”„ë¡œì„¸ìŠ¤/íŒŒë“œ
class AIWorker {
  async processAnalyzeJob(job: Job) {
    const { diaryId, content } = job.data;
    
    // LangGraph ì²´ì¸ ì‹¤í–‰
    const result = await this.langGraphChain.invoke({
      content,
      diaryId
    });
    
    // ê²°ê³¼ ì €ì¥
    await this.saveResult(diaryId, result);
    
    // ì›¹ì†Œì¼“ìœ¼ë¡œ ì•Œë¦¼
    await this.notifyUser(job.data.userId, result);
  }
}
```

## ğŸ“Š LangGraph ìƒíƒœ ê´€ë¦¬

### 1. ë¶„ì‚° í™˜ê²½ì—ì„œì˜ ìƒíƒœ ì €ì¥

```typescript
import { StateGraph, Annotation } from "@langchain/langgraph";
import { RedisCheckpointer } from "./redis-checkpointer";

// ìƒíƒœ ìŠ¤í‚¤ë§ˆ ì •ì˜
const DiaryAnalysisState = Annotation.Root({
  diaryId: Annotation<string>,
  content: Annotation<string>,
  extractedTopics: Annotation<string[]>,
  identifiedEmotions: Annotation<string[]>,
  strengths: Annotation<string[]>,
  insights: Annotation<string[]>,
  feedback: Annotation<string>,
  error: Annotation<string | null>
});

// Redis ê¸°ë°˜ ì²´í¬í¬ì¸í„° (ìƒíƒœ ì €ì¥)
export class RedisCheckpointer {
  constructor(private redis: Redis) {}

  async save(threadId: string, checkpoint: any) {
    await this.redis.set(
      `checkpoint:${threadId}`,
      JSON.stringify(checkpoint),
      'EX', 86400 // 24ì‹œê°„ TTL
    );
  }

  async load(threadId: string) {
    const data = await this.redis.get(`checkpoint:${threadId}`);
    return data ? JSON.parse(data) : null;
  }

  async list(threadId: string) {
    // ì²´í¬í¬ì¸íŠ¸ íˆìŠ¤í† ë¦¬
    const keys = await this.redis.keys(`checkpoint:${threadId}:*`);
    return Promise.all(
      keys.map(async (key) => {
        const data = await this.redis.get(key);
        return JSON.parse(data!);
      })
    );
  }
}

// LangGraph ì›Œí¬í”Œë¡œìš°
export function createDiaryAnalysisGraph() {
  const workflow = new StateGraph(DiaryAnalysisState)
    .addNode("extract_topics", extractTopicsNode)
    .addNode("analyze_emotions", analyzeEmotionsNode)
    .addNode("identify_strengths", identifyStrengthsNode)
    .addNode("generate_insights", generateInsightsNode)
    .addNode("create_feedback", createFeedbackNode)
    .addEdge("__start__", "extract_topics")
    .addEdge("extract_topics", "analyze_emotions")
    .addEdge("analyze_emotions", "identify_strengths")
    .addEdge("identify_strengths", "generate_insights")
    .addEdge("generate_insights", "create_feedback")
    .addEdge("create_feedback", "__end__");

  // Redis ì²´í¬í¬ì¸í„° ì‚¬ìš©
  const checkpointer = new RedisCheckpointer(redis);
  
  return workflow.compile({ 
    checkpointer,
    // ì¸í„°ëŸ½íŠ¸ ê°€ëŠ¥í•œ ë…¸ë“œ ì„¤ì •
    interruptBefore: ["generate_insights"], 
  });
}
```

### 2. ë…¸ë“œ êµ¬í˜„ ì˜ˆì‹œ

```typescript
// ê° ë…¸ë“œëŠ” ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•´ì•¼ í•¨
async function extractTopicsNode(state: typeof DiaryAnalysisState.State) {
  const { content } = state;
  
  try {
    // í”„ë¡¬í”„íŠ¸ ìºì‹±
    const cacheKey = `topics:${createHash(content)}`;
    const cached = await redis.get(cacheKey);
    
    if (cached) {
      return { extractedTopics: JSON.parse(cached) };
    }
    
    // AI í˜¸ì¶œ
    const response = await openai.chat.completions.create({
      model: "gpt-3.5-turbo", // ë¹„ìš© ì ˆì•½
      messages: [
        {
          role: "system",
          content: "ì¼ê¸°ì—ì„œ ì£¼ìš” ì£¼ì œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”. JSON ë°°ì—´ë¡œ ì‘ë‹µí•˜ì„¸ìš”."
        },
        {
          role: "user",
          content
        }
      ],
      temperature: 0.3,
      max_tokens: 200,
      response_format: { type: "json_object" }
    });
    
    const topics = JSON.parse(response.choices[0].message.content!).topics;
    
    // ê²°ê³¼ ìºì‹±
    await redis.setex(cacheKey, 3600, JSON.stringify(topics));
    
    return { extractedTopics: topics };
  } catch (error) {
    console.error("Topic extraction failed:", error);
    return { error: "Failed to extract topics" };
  }
}

async function identifyStrengthsNode(state: typeof DiaryAnalysisState.State) {
  const { content, extractedTopics, identifiedEmotions } = state;
  
  // ì´ì „ ë‹¨ê³„ ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ìµœì í™”
  const prompt = `
ì¼ê¸° ë‚´ìš©ê³¼ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±ìì˜ ê°•ì ì„ ì°¾ì•„ì£¼ì„¸ìš”.

ì¼ê¸° ë‚´ìš©: ${content}
ì£¼ìš” ì£¼ì œ: ${extractedTopics.join(", ")}
ê°ì • ìƒíƒœ: ${identifiedEmotions.join(", ")}

ë‹¤ìŒ ê´€ì ì—ì„œ ê°•ì ì„ ì°¾ì•„ì£¼ì„¸ìš”:
1. ë¬¸ì œ í•´ê²° ëŠ¥ë ¥
2. ëŒ€ì¸ ê´€ê³„ ìŠ¤í‚¬
3. ìê¸° ê´€ë¦¬ ëŠ¥ë ¥
4. ì°½ì˜ì„±ê³¼ í˜ì‹ 
5. ë¦¬ë”ì‹­ê³¼ í˜‘ì—…

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{
  "strengths": ["ê°•ì 1", "ê°•ì 2", ...],
  "evidence": {"ê°•ì 1": "êµ¬ì²´ì  ì¦ê±°", ...}
}
`;

  const response = await openai.chat.completions.create({
    model: "gpt-4-turbo-preview",
    messages: [
      { role: "system", content: "ë‹¹ì‹ ì€ ì „ë¬¸ ì»¤ë¦¬ì–´ ì½”ì¹˜ì…ë‹ˆë‹¤." },
      { role: "user", content: prompt }
    ],
    temperature: 0.5,
    response_format: { type: "json_object" }
  });
  
  const result = JSON.parse(response.choices[0].message.content!);
  
  return { 
    strengths: result.strengths,
    // ì¦ê±°ëŠ” ë³„ë„ë¡œ ì €ì¥
    strengthEvidence: result.evidence 
  };
}
```

## ğŸš€ ì„±ëŠ¥ ìµœì í™” ì „ëµ

### 1. í”„ë¡¬í”„íŠ¸ ìºì‹±

```typescript
export class PromptCache {
  private redis: Redis;
  private ttl: number = 3600; // 1ì‹œê°„
  
  // ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ ìºì‹±
  async getSimilar(content: string, threshold: number = 0.95) {
    // ì„ë² ë”© ìƒì„±
    const embedding = await this.generateEmbedding(content);
    
    // Redisì—ì„œ ìœ ì‚¬í•œ ì„ë² ë”© ê²€ìƒ‰
    const similar = await this.redis.call(
      'FT.SEARCH',
      'idx:embeddings',
      `*=>[KNN 10 @embedding $vec AS score]`,
      'PARAMS', '2', 'vec', embedding,
      'RETURN', '2', 'content', 'result',
      'SORTBY', 'score',
      'LIMIT', '0', '1'
    );
    
    if (similar && similar[1]?.score > threshold) {
      return JSON.parse(similar[1].result);
    }
    
    return null;
  }
  
  async set(content: string, result: any) {
    const embedding = await this.generateEmbedding(content);
    const key = `cache:${createHash(content)}`;
    
    await this.redis.hset(key, {
      content,
      embedding: Buffer.from(embedding),
      result: JSON.stringify(result),
      timestamp: Date.now()
    });
    
    await this.redis.expire(key, this.ttl);
  }
}
```

### 2. ë°°ì¹˜ ì²˜ë¦¬

```typescript
export class BatchProcessor {
  private batchSize = 10;
  private batchTimeout = 5000; // 5ì´ˆ
  private pendingBatch: Array<{
    content: string;
    resolve: (result: any) => void;
    reject: (error: any) => void;
  }> = [];
  
  async process(content: string): Promise<any> {
    return new Promise((resolve, reject) => {
      this.pendingBatch.push({ content, resolve, reject });
      
      if (this.pendingBatch.length >= this.batchSize) {
        this.processBatch();
      } else if (this.pendingBatch.length === 1) {
        // ì²« ë²ˆì§¸ ì•„ì´í…œì´ë©´ íƒ€ì´ë¨¸ ì‹œì‘
        setTimeout(() => this.processBatch(), this.batchTimeout);
      }
    });
  }
  
  private async processBatch() {
    const batch = this.pendingBatch.splice(0, this.batchSize);
    if (batch.length === 0) return;
    
    try {
      // ë°°ì¹˜ë¡œ ì²˜ë¦¬ (API í˜¸ì¶œ ìˆ˜ ê°ì†Œ)
      const results = await this.batchAnalyze(
        batch.map(item => item.content)
      );
      
      batch.forEach((item, index) => {
        item.resolve(results[index]);
      });
    } catch (error) {
      batch.forEach(item => item.reject(error));
    }
  }
  
  private async batchAnalyze(contents: string[]) {
    const response = await openai.chat.completions.create({
      model: "gpt-3.5-turbo",
      messages: [
        {
          role: "system",
          content: "ë‹¤ìŒ ì¼ê¸°ë“¤ì„ ê°ê° ë¶„ì„í•˜ì„¸ìš”. JSON ë°°ì—´ë¡œ ì‘ë‹µí•˜ì„¸ìš”."
        },
        {
          role: "user",
          content: JSON.stringify(contents)
        }
      ],
      temperature: 0.5
    });
    
    return JSON.parse(response.choices[0].message.content!);
  }
}
```

## ğŸ“¦ Kubernetes ë°°í¬ êµ¬ì„±

### 1. AI ì›Œì»¤ Deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ai-worker
spec:
  replicas: 2  # GPU ë¹„ìš© ê³ ë ¤
  selector:
    matchLabels:
      app: ai-worker
  template:
    metadata:
      labels:
        app: ai-worker
    spec:
      containers:
      - name: worker
        image: cushion/ai-worker:latest
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
            # nvidia.com/gpu: 1  # GPU ì‚¬ìš© ì‹œ
          limits:
            memory: "4Gi"
            cpu: "2000m"
        env:
        - name: WORKER_CONCURRENCY
          value: "5"  # ë™ì‹œ ì²˜ë¦¬ ì‘ì—… ìˆ˜
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: ai-secrets
              key: openai-api-key
        - name: REDIS_URL
          valueFrom:
            secretKeyRef:
              name: redis-secrets
              key: url
        - name: LANGGRAPH_CHECKPOINT_TTL
          value: "86400"  # 24ì‹œê°„
        - name: AI_CACHE_TTL
          value: "3600"   # 1ì‹œê°„
        volumeMounts:
        - name: model-cache
          mountPath: /app/models  # ë¡œì»¬ ëª¨ë¸ ì‚¬ìš© ì‹œ
      volumes:
      - name: model-cache
        persistentVolumeClaim:
          claimName: ai-model-cache
```

### 2. í ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§

```yaml
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: ai-worker-scaler
spec:
  scaleTargetRef:
    name: ai-worker
  minReplicaCount: 1
  maxReplicaCount: 10
  triggers:
  - type: redis
    metadata:
      address: redis:6379
      listName: "ai-queue"
      listLength: "10"  # íì— 10ê°œ ì´ìƒ ìŒ“ì´ë©´ ìŠ¤ì¼€ì¼ë§
```

## ğŸ” ëª¨ë‹ˆí„°ë§ ë° ê´€ì°°ì„±

### 1. LangGraph ì‹¤í–‰ ì¶”ì 

```typescript
import { LangGraphTracer } from "./tracing";

export class TracedLangGraphChain {
  private tracer: LangGraphTracer;
  
  async invoke(input: any) {
    const traceId = generateTraceId();
    const span = this.tracer.startSpan("langgraph.invoke", {
      traceId,
      input: JSON.stringify(input).slice(0, 1000), // ë¡œê·¸ í¬ê¸° ì œí•œ
    });
    
    try {
      // ê° ë…¸ë“œ ì‹¤í–‰ ì¶”ì 
      const config = {
        callbacks: [
          {
            handleLLMStart: (llm: any, prompts: string[]) => {
              span.addEvent("llm.start", {
                model: llm.model,
                promptLength: prompts.join("").length,
              });
            },
            handleLLMEnd: (output: any) => {
              span.addEvent("llm.end", {
                tokensUsed: output.llmOutput?.tokenUsage?.totalTokens,
              });
            },
            handleChainError: (error: any) => {
              span.recordException(error);
            },
          },
        ],
      };
      
      const result = await this.chain.invoke(input, config);
      
      span.setAttributes({
        "langgraph.success": true,
        "langgraph.nodes_executed": result.metadata?.nodesExecuted,
      });
      
      return result;
    } catch (error) {
      span.setAttributes({
        "langgraph.success": false,
        "langgraph.error": error.message,
      });
      throw error;
    } finally {
      span.end();
    }
  }
}
```

### 2. ë¹„ìš© ì¶”ì 

```typescript
export class CostTracker {
  private metrics: MetricsClient;
  
  // ëª¨ë¸ë³„ í† í° ë¹„ìš© (USD per 1K tokens)
  private costs = {
    "gpt-3.5-turbo": { input: 0.0005, output: 0.0015 },
    "gpt-4-turbo-preview": { input: 0.01, output: 0.03 },
    "gpt-4": { input: 0.03, output: 0.06 },
  };
  
  trackUsage(model: string, usage: any) {
    const cost = this.calculateCost(model, usage);
    
    // Prometheus ë©”íŠ¸ë¦­
    this.metrics.increment("ai_tokens_total", usage.totalTokens, {
      model,
      type: "total",
    });
    
    this.metrics.increment("ai_cost_usd", cost, {
      model,
    });
    
    // ì¼ì¼ ì˜ˆì‚° í™•ì¸
    this.checkDailyBudget(cost);
  }
  
  private calculateCost(model: string, usage: any): number {
    const rates = this.costs[model];
    if (!rates) return 0;
    
    const inputCost = (usage.promptTokens / 1000) * rates.input;
    const outputCost = (usage.completionTokens / 1000) * rates.output;
    
    return inputCost + outputCost;
  }
  
  private async checkDailyBudget(cost: number) {
    const today = new Date().toISOString().split('T')[0];
    const dailyTotal = await redis.incrby(`ai:cost:${today}`, cost * 100);
    
    const budgetLimit = parseFloat(process.env.AI_DAILY_BUDGET || "100");
    
    if (dailyTotal / 100 > budgetLimit) {
      // ì•Œë¦¼ ë°œì†¡
      await this.notifyBudgetExceeded(dailyTotal / 100, budgetLimit);
      
      // ì˜µì…˜: ì„œë¹„ìŠ¤ ì œí•œ
      if (process.env.AI_BUDGET_ENFORCEMENT === "strict") {
        throw new Error("Daily AI budget exceeded");
      }
    }
  }
}
```

## ğŸ›¡ ì—ëŸ¬ ì²˜ë¦¬ ë° í´ë°±

### 1. ê³„ì¸µì  í´ë°± ì „ëµ

```typescript
export class AIServiceWithFallback {
  async analyze(content: string): Promise<AnalysisResult> {
    // 1ì°¨: ìºì‹œ í™•ì¸
    const cached = await this.cache.getSimilar(content);
    if (cached) return cached;
    
    // 2ì°¨: ë©”ì¸ ëª¨ë¸ (GPT-4)
    try {
      return await this.analyzeWithGPT4(content);
    } catch (error) {
      console.error("GPT-4 failed:", error);
      
      // 3ì°¨: í´ë°± ëª¨ë¸ (GPT-3.5)
      try {
        return await this.analyzeWithGPT35(content);
      } catch (error) {
        console.error("GPT-3.5 failed:", error);
        
        // 4ì°¨: ë¡œì»¬ ëª¨ë¸ (Ollama)
        try {
          return await this.analyzeWithLocalModel(content);
        } catch (error) {
          console.error("Local model failed:", error);
          
          // 5ì°¨: ê¸°ë³¸ ê·œì¹™ ê¸°ë°˜ ë¶„ì„
          return this.basicAnalysis(content);
        }
      }
    }
  }
  
  private async analyzeWithLocalModel(content: string) {
    // Ollama ë˜ëŠ” ë‹¤ë¥¸ ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©
    const response = await fetch("http://ollama:11434/api/generate", {
      method: "POST",
      body: JSON.stringify({
        model: "llama2",
        prompt: this.buildPrompt(content),
      }),
    });
    
    return this.parseResponse(await response.json());
  }
  
  private basicAnalysis(content: string): AnalysisResult {
    // ê·œì¹™ ê¸°ë°˜ ê¸°ë³¸ ë¶„ì„
    return {
      strengths: this.extractBasicStrengths(content),
      emotions: this.detectBasicEmotions(content),
      insights: ["ì¼ê¸°ë¥¼ ì‘ì„±í•˜ì‹  ê²ƒë§Œìœ¼ë¡œë„ í›Œë¥­í•œ ìê¸° ì„±ì°°ì…ë‹ˆë‹¤."],
      confidence: 0.3,
    };
  }
}
```

### 2. ì„œí‚· ë¸Œë ˆì´ì»¤ íŒ¨í„´

```typescript
import CircuitBreaker from 'opossum';

export class ResilientAIService {
  private breaker: CircuitBreaker;
  
  constructor() {
    const options = {
      timeout: 30000,      // 30ì´ˆ íƒ€ì„ì•„ì›ƒ
      errorThresholdPercentage: 50,  // 50% ì‹¤íŒ¨ ì‹œ ì—´ë¦¼
      resetTimeout: 30000,  // 30ì´ˆ í›„ ì¬ì‹œë„
      volumeThreshold: 10,  // ìµœì†Œ 10ê°œ ìš”ì²­ í›„ íŒë‹¨
    };
    
    this.breaker = new CircuitBreaker(
      this.callOpenAI.bind(this),
      options
    );
    
    // ì„œí‚· ë¸Œë ˆì´ì»¤ ì´ë²¤íŠ¸ ëª¨ë‹ˆí„°ë§
    this.breaker.on('open', () => {
      console.error('Circuit breaker is OPEN - AI service unavailable');
      this.metrics.increment('circuit_breaker_open', 1, { service: 'openai' });
    });
    
    this.breaker.on('halfOpen', () => {
      console.log('Circuit breaker is HALF-OPEN - testing AI service');
    });
    
    this.breaker.fallback(() => {
      // í´ë°± ë¡œì§
      return this.getCachedOrDefaultResponse();
    });
  }
  
  async analyze(content: string) {
    return this.breaker.fire(content);
  }
}
```

## ğŸ” ë³´ì•ˆ ê³ ë ¤ì‚¬í•­

### 1. API í‚¤ ë¡œí…Œì´ì…˜

```typescript
export class APIKeyManager {
  private keys: string[] = [];
  private currentIndex = 0;
  
  constructor() {
    // í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì—¬ëŸ¬ í‚¤ ë¡œë“œ
    this.keys = process.env.OPENAI_API_KEYS?.split(',') || [];
    
    // ì£¼ê¸°ì  ë¡œí…Œì´ì…˜
    setInterval(() => this.rotate(), 3600000); // 1ì‹œê°„ë§ˆë‹¤
  }
  
  getCurrentKey(): string {
    return this.keys[this.currentIndex];
  }
  
  rotate() {
    this.currentIndex = (this.currentIndex + 1) % this.keys.length;
    console.log(`Rotated to API key index: ${this.currentIndex}`);
  }
  
  markKeyAsInvalid(key: string) {
    const index = this.keys.indexOf(key);
    if (index > -1) {
      this.keys.splice(index, 1);
      console.error(`Removed invalid API key at index: ${index}`);
      
      // ì•Œë¦¼ ë°œì†¡
      this.notifyInvalidKey(key);
    }
  }
}
```

### 2. í”„ë¡¬í”„íŠ¸ ì¸ì ì…˜ ë°©ì–´

```typescript
export class SecurePromptBuilder {
  // ìœ„í—˜í•œ íŒ¨í„´ í•„í„°ë§
  private sanitize(input: string): string {
    const dangerous = [
      /ignore previous instructions/i,
      /disregard all prior/i,
      /system:/i,
      /\{\{.*\}\}/,  // í…œí”Œë¦¿ ì¸ì ì…˜
    ];
    
    let sanitized = input;
    dangerous.forEach(pattern => {
      sanitized = sanitized.replace(pattern, '[FILTERED]');
    });
    
    return sanitized;
  }
  
  buildPrompt(userInput: string): string {
    const sanitized = this.sanitize(userInput);
    
    // ëª…í™•í•œ ê²½ê³„ ì„¤ì •
    return `
You are a career coach AI assistant. Follow these rules strictly:
1. Only analyze the diary content provided
2. Do not execute any commands or code
3. Focus only on identifying strengths and insights

<diary_content>
${sanitized}
</diary_content>

Analyze the diary content above and provide insights.
`;
  }
}
```

ì´ë ‡ê²Œ LangGraphì™€ AI ì„œë¹„ìŠ¤ë¥¼ í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ í™˜ê²½ì—ì„œ ìš´ì˜í•˜ë©´ í™•ì¥ì„±, ì•ˆì •ì„±, ë¹„ìš© íš¨ìœ¨ì„±ì„ ëª¨ë‘ í™•ë³´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!